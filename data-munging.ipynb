{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook handles **most** of the downloading, loading, parsing, joining, and saving the combined [PLUTO](http://www1.nyc.gov/site/planning/data-maps/open-data/dwn-pluto-mappluto.page), [Rolling Sales](https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page), and [RPAD](https://www1.nyc.gov/site/finance/taxes/property-assessments.page) datasets. A bit of the work is done out-of-band&mdash;you'll see why.\n",
    "\n",
    "Note that in all of these datasets the unit of measurement is a borough-block-lot combination, each of which maps uniquely to an individual property. That property may be a building (in the case of actual buildings, small homes, and co-ops) or it may be an apartment (in the case of condominiums). None of the three datasets provide information on the size of individual apartments, though assessment values and market values are present in the RPAD and Rolling Sales datasets, respectively. Therefore there is no way to map information on apartment values. These records are removed at a later step. The focus of this project therefore is on whole-building values.\n",
    "\n",
    "Now on to the datasets.\n",
    "\n",
    "RPAD is a record of the assessed value, as determined by the New York City Department of Finance and used for taxation purposes, of every building and apartment in New York City. These assessed values are by the department's own admission generally a few cycles or years behind the trend of the market, but are nevertheless a valuable and almost complete record of all property values in New York City.\n",
    "\n",
    "The PLUTO dataset agglomerates a large number of datasets published by various agencies in New York City into a single master record for categorical information on every property in New York City. It gives RPAD and Rolling Sales information a rich context.\n",
    "\n",
    "Rolling Sales contains the actual market prices of all buildings sold in New York City in the last twelve months. Past that horizon real estate market trends make this data less concrete. Since market value is our target variable, Rolling Sales contains ground truths about what we would like to model. On the other hand it also contains a significant amount of noise, dealing with which will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling sales data is provided as lightly formatted `xls` files (Excel):\n",
    "\n",
    "![alt text](./rolling-sales-data-excel-screencap.png \"\")\n",
    "\n",
    "Luckily the extraneous details are easily patched up post-import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:25<00:00,  5.25s/it]\n"
     ]
    }
   ],
   "source": [
    "rolling_sales_data = dict()\n",
    "rolling_sales_data_key_pairs = {'Manhattan': 'manhattan',\n",
    "                                'Brooklyn': 'brooklyn',\n",
    "                                'Queens': 'queens',\n",
    "                                'Bronx': 'bronx',\n",
    "                                'Staten Island': 'statenisland'}\n",
    "for b_k, b_xls in tqdm(list(rolling_sales_data_key_pairs.items())):\n",
    "    borough_rsd = pd.read_excel('https://www1.nyc.gov/assets/finance/downloads/pdf/rolling_sales/rollingsales_{0}.xls'.format(b_xls))\n",
    "    borough_rsd.columns = borough_rsd.iloc[3].values\n",
    "    borough_rsd = borough_rsd[4:]\n",
    "    rolling_sales_data[b_k] = borough_rsd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLUTO data is provided as borough-denominated `csv` files packaged into a `zip`. The following code bit unpacks the data and rekeys the file (`QN.csv`, `BK.csv`, `BX.csv`, `SI.csv`, `Mn.csv`) to match the lexicon used for the rolling sales data, above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:01<00:05,  1.34s/it]/Users/Honors/anaconda/envs/nyc-buildings/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (4,6,7,8,10,11,50,52,53,77,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      " 40%|████      | 2/5 [00:04<00:05,  1.98s/it]/Users/Honors/anaconda/envs/nyc-buildings/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (6,7,8,10,27,50,52,53,77,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      " 60%|██████    | 3/5 [00:12<00:07,  3.68s/it]/Users/Honors/anaconda/envs/nyc-buildings/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (6,7,10,11,50,52,53,77,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "100%|██████████| 5/5 [00:24<00:00,  4.58s/it]\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('http://www1.nyc.gov/assets/planning/download/zip/data-maps/open-data/nyc_pluto_15v1.zip')\n",
    "pluto_key_pairs = {'Manhattan': 'Mn.csv',\n",
    "                   'Brooklyn': 'BK.csv',\n",
    "                   'Bronx': 'BX.csv',\n",
    "                   'Staten Island': 'SI.csv',\n",
    "                   'Queens': 'QN.csv'}\n",
    "pluto_data = dict()\n",
    "for b_k, b_csv in tqdm(list(pluto_key_pairs.items())):\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content)) as ar:\n",
    "        borough_pluto = pd.read_csv(ar.open(b_csv))\n",
    "        pluto_data[b_k] = borough_pluto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the PLUTO and Rolling Sales datasets provide their data on a per-borough basis, and since we would like to study the entire city, we must now flatten each set of tables into two big tables. Along the way we encode an additional `Borough` column, to preserve information.\n",
    "\n",
    "**Note**: the `Borough` mapping is a new column in the rolling sales dataset; a prexisting (numerically encoded) `BOROUGH` column is removed. The mapping overlays and replaces an older (acronym-encoded) column in the `PLUTO` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:08<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "rolling_sales_agglom = pd.DataFrame(columns=rolling_sales_data['Manhattan'].columns)\n",
    "pluto_data_agglom = pd.DataFrame(columns=pluto_data['Manhattan'].columns)\n",
    "for b_k in tqdm(pluto_key_pairs.keys()):\n",
    "    pluto_data[b_k]['Borough'] = rolling_sales_data[b_k]['Borough'] = b_k\n",
    "    rolling_sales_agglom = pd.concat([rolling_sales_agglom, rolling_sales_data[b_k]], ignore_index=True)\n",
    "    pluto_data_agglom = pd.concat([pluto_data_agglom, pluto_data[b_k]], ignore_index=True)\n",
    "del rolling_sales_agglom['BOROUGH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPAD data is the trickiest. RPAD is split into two files, one for properties in tax class 1 (single, double, and triple -family homes) and one for those in tax classes 2/3/4 (everything else). These are provided as compressed `zip` files containing `mdb` database files (for Microsoft Access).\n",
    "\n",
    "I handled this the easiest way I could: by jumping on a Windows Desktop, opening the databases, opening them in Microsoft Access, and exporting them to comma-delimited `txt` files using the GUI (be sure to check the box that asks if you'd like to include the field name in the first column!). There are automated ways of doing this but they're sure to be painful and this is easiest. The resulting files can then be read by `pandas`.\n",
    "\n",
    "Check out [mdbtools](https://github.com/brianb/mdbtools) (the [homebrew installation](http://brewformulas.org/Mdbtools) for Mac OS X) for \\*nix systems (not sure what the easiest way to get it on Linux is). If you're working on Windows and don't have access to Microsoft Access (heh), I'm not sure but I think [pyodbc](https://github.com/mkleehammer/pyodbc) will work.\n",
    "\n",
    "The odd filenames come from the odd names the underlying database for its tables. They stand for tax class 1 and tax classes 2/3/4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both-tabs-and-commas-screencap.png      nyc_buildings.csv\r\n",
      "data-munging.ipynb                      nyc_pluto.csv\r\n",
      "data-scribbles.ipynb                    nyc_rolling_sales.csv\r\n",
      "nyc_building_sales.csv                  rolling-sales-data-excel-screencap.png\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rpad_data_agglom = pd.concat([pd.read_csv(\"tc1.txt\"), pd.read_csv(\"tc234.txt\")], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we flatten these two files into one by performing an outer join on the `(Borough, Block, Lot)` unique key (this is *supposedly* [standard](http://www1.nyc.gov/nyc-resources/service/1232/borough-block-lot-bbl-lookup) but turned out to be far far harder than anticipated&mdash;keep reading for more on why).\n",
    "\n",
    "In order to do this we first remap the column names in the rolling sales dataset from `SPACED ALL CAPS` to `NoSpaceCamelCase` (as used by `PLUTO`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rolling_sales_agglom.columns = [c.title().replace(' ', '') for c in list(rolling_sales_agglom.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every record in the Rolling Sales dataset represents an actual sale. A large number of records are of what are effectively deed transfers: sales of a building for either `$0` or occassionally some paltry some, usually between family members, sometimes as a part of a contract, and so on. Since these records do not encode any actual information they should be dropped.\n",
    "\n",
    "`$1000` is arbitrarily chosen as a cutoff value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rolling_sales_agglom = rolling_sales_agglom[rolling_sales_agglom['SalePrice'] > 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rolling Sales dataset contains information on the sale of both individual apartments and of whole buildings, but does not contain the square footage of the apartment sales in the data. Since we need this for calculating standardized value per square foot, we have to throw out apartments entirely, unfortunately, from the dataset.\n",
    "\n",
    "To do this we remove entries with no defined `LandSquareFeet` (no apartments have this value in any of the datasets, weirdly enough) and restrict apartment number to a lack of one, which is weirdly `'            '`, as in, a long space. See [Cleaning](#Cleaning) for more on why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rolling_sales_agglom.ix[0, :]['ApartmentNumber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rs_a_f = rolling_sales_agglom[rolling_sales_agglom['ApartmentNumber'] == '            ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_a_f = rs_a_f[rs_a_f['LandSquareFeet'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our assumption going into this project was that the `Borough`-`Block`-`Lot` columns present in both PLUTO and the Rolling Sales dataset (these are an identifier used for taxation purposes) are equivalent, allowing us to use this combo as a unique key for a join. However, this assumption turns out to be incorrect. Both PLUTO and Rolling Sales break this assumption, and in different ways.\n",
    "\n",
    "The PLUTO dataset contains information on the square footage of entire buildings, but not on that of individual units. This is because instead of using the usual Tax-Block-Lot system PLUTO defines and uses its own Lot configuration, painting over condominiums with multiple lots in a single building by merging them into one Lot. This is useful for geospatial visualization but voids the use of the dataset for ordinary apartment-based residential lookup: the Rolling Sales dataset contains sales information on individual apartments which, as a consequence of this tweak, cannot be mapped to any of the records in PLUTO.\n",
    "\n",
    "That doesn't really bother us though&mdash;apartments were already disqualified by the fact that nowhere is there a data stream for their size, without which their price cannot be standardized.\n",
    "\n",
    "Rolling Sales breaks the uniqueness assumption by, bizarrely enough, allowing the sale of sub-units within a single property block. This is really disorienting: isn't the whole point of having a single identifier that it's atomic? Apparently in the city's eyes, no. And thus it is that individual sales of components of a property are all recorded seperately!\n",
    "\n",
    "How do we handle this? `Rolling Sales` contains both the `SalePrice` and the square footage of the property sold. Instead of working with `SalePrice` directly, we will compute an average `MarketValueSqFt` based on the sum of the `SalePrice` fields divided by the sum of the `BldgArea` fields for each unique `Borough`-`Block`-`Lot` combination in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function is a doozy! Here' what it does, step-by-step:\n",
    "# 1. Select our desired slice of the variables from the rolling sales data.\n",
    "# 2. Aggregate by Borough-Block-Lot, creating a groupby object.\n",
    "# 3. Merge the non-key variables via summation, converting the groupby object to the hierarchical DataFrame.\n",
    "# 4. Reset the index to shake off the hierarchical index and recreate a simple numerical one.\n",
    "# 5. Assign all of that to rs_a_ff.\n",
    "rs_a_ff = rs_a_f[['Borough', 'Block', 'Lot', 'SalePrice', 'LandSquareFeet']].groupby(by=['Borough', 'Block', 'Lot']).sum().reset_index()\n",
    "# Now all that's left is to create a new column for market value by broadcasting division.\n",
    "rs_a_ff['MarketValueSqFt'] = rs_a_ff['SalePrice'] / rs_a_ff['LandSquareFeet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records did we fold this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(rs_a_f) - len(rs_a_ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rolling_pluto = pd.merge(rs_a_ff, pluto_data_agglom,\n",
    "                         how='outer', on=['Borough', 'Block', 'Lot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not immediately apparent why, but this resulted in slightly more records than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(rolling_pluto) - len(pluto_data_agglom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we be worried about 71 records amongst 859535 of them? Probably not. A banding procedure I use later on probably whipes most of these remaining pressure points out.\n",
    "\n",
    "`SalePrice`-populated mergers fired correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(rs_a_ff) - len(rolling_pluto[rolling_pluto['SalePrice'] >= 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we want to merge in the RPAD dataset. RPAD has a borough column much like the Rolling Sales dataset&mdash;numbers instead of the borough names that we need to join it to `rolling_pluto`. Since the data isn't provided on a borough-denominated basis, in this case we'll have to explicitly map the `BORO` numerical column to a name value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpad_key_pairs = {1.0: 'Manhattan',\n",
    "                  2.0: 'Bronx',\n",
    "                  3.0: 'Brooklyn',\n",
    "                  4.0: 'Queens',\n",
    "                  5.0: 'Staten Island',\n",
    "                 }\n",
    "rpad_data_agglom['Borough'] = rpad_data_agglom['BORO'].apply(lambda n: rpad_key_pairs[n])\n",
    "del rpad_data_agglom['BORO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this dataset has its own format for variables, in this case the format is `ALL_CAPS_SPACERS`. Again we convert to `CamelCase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rpad_data_agglom.columns = [c.title().replace('_', '') for c in list(rpad_data_agglom.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rpad_data_agglom.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables of interest in RPAD (what we want to keep after the merge) are:\n",
    "\n",
    "* Borough (merge key).\n",
    "* Block (merge key).\n",
    "* Lot (merge key).\n",
    "* CurFvT &mdash; Current market value, total, as of assessment sometime in 2015 and as determined by the city. This is the Finance office's best guess as to the value of this property.\n",
    "* NewFvT &mdash; New market value, total, as of prospective assessment in early 2016 and as determined by the city. Note that the Finance office lags, by its own admission, generally a cycle or two behind the movements of the real estate market.\n",
    "* CuravtA &mdash; current assessed value, total, as of assessment sometime in 2015 as determined by the city. Assessed value is computed using a complex and obtruse formulaic determination of rental market value. The very similar `Curavt` is a similar statistics which is rebalanced to increase no more than `20%` a year, per tax laws ([source](https://www1.nyc.gov/site/finance/taxes/property-determining-your-transitional-assessed-value.page)); it is excluded because `CuravtA` is, therefore, better correlated with actual value. `Curavt`, not market value, is what is used by the city to assess residential tax.\n",
    "\n",
    "Tentative and final assessment information excluded because it is incomplete pending the release of this dataset for the most recent financial year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rpad_columns_of_interest = ['Borough', 'Block', 'Lot', 'CurFvT', 'NewFvT', 'CuravtA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rpad_data_agglom[rpad_columns_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = rpad_data_agglom.groupby(by=['Borough', 'Block', 'Lot']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counts = counts.sort_values(by='CurFvT', ascending=False)\n",
    "counts[counts['Bble'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK now we can merg&mdash;nope!\n",
    "\n",
    "Merging these two datasets as-is created >2800 extra entries, indicating that `Borough`-`Block`-`Lot` *still* wasn't unique somewhere.\n",
    "\n",
    "Regrouping the `RPAD` data according to this schema reveals that this is indeed the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpad_data_agglom.groupby(by=['Borough', 'Block', 'Lot']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in on one of the cases shows why&mdash;easements are all listed seperately within the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpad_data_agglom[(rpad_data_agglom['Borough'] == 'Bronx') &\n",
    "                 (rpad_data_agglom['Block'] == 2260) &\n",
    "                 (rpad_data_agglom['Lot'] == 62)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok...let's fix this one too. Easements indicate encumerances on the property by others&mdash;usually the city having a subway track lying on a piece of your land or something similar (also, this property with six easements isn't even the winner, there's another somewhere with ten!) #useless-factiods.\n",
    "\n",
    "Easements have no property value, so we can simply remove the entries corresponding to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpad_data_agglom_f = rpad_data_agglom[rpad_data_agglom['Ease'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the merge. Yes! It's really happening!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rolling_pluto_rpad = pd.merge(rolling_pluto, rpad_data_agglom_f[rpad_columns_of_interest],\n",
    "                              how='inner', on=['Borough', 'Block', 'Lot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch of records are lost in the join, which is good: the rolling pluto dataset is cleaner by-and-by than the RPAD one, so we ought to be refusing bad records, not creating new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(rolling_pluto_rpad) - len(rolling_pluto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the size of the loss is small enough to be acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(len(rolling_pluto_rpad) - len(rolling_pluto)) / len(rolling_pluto_rpad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BldgArea` is an important variable for us, but quite a few records are missing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rolling_pluto_rpad['BldgArea'].value_counts()[0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no way to interpolate that information so we'll just drop them. Ditto with records missing an `Address`&mdash;these are usually malformed records or records of places belonging to the city or to the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rolling_pluto_rpad = rolling_pluto_rpad[rolling_pluto_rpad['BldgArea'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries in the PLUTO dataset are missing an address. Certain city-owned or public spaces&mdash;parks, for example&mdash;don't get one because they're not even technically buildings. The inner join with RPAD *appears* to have taken care of this problem, but for safety's sake let's explicitly disallow it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rolling_pluto_rpad = rolling_pluto_rpad[rolling_pluto_rpad['Address'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we hit the next issue. At the copy step during our merge process `pandas` complained that many of the columns that we are working with have a mixed `dtype`:\n",
    "\n",
    "> `DtypeWarning: Columns (4,6,7,8,10,11,50,52,53,77,79) have mixed types. Specify dtype option on import or set low_memory=False.`\n",
    "\n",
    "After casting these columns using `np.astyle(float)` failed I wrote a `try-else` block and caught on to a sentinal value in `CT2010` of `'       '`, much like the strange spacer above, that was messing with the column type. The other columns also have other variable length spacers like this floating around in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(rolling_pluto_rpad[rolling_pluto_rpad['CT2010'] == '       '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(rolling_pluto_rpad[rolling_pluto_rpad['CB2010'] == '     '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a mess. This problem occurs in both float columns and string columns and occurs with strings of variable length, making it difficult to pick out how to cast to get rid of it. The problem isn't immediately evident from `pandas` displays because `pandas` strips empty space from its display, but if you `to_csv()` the file and check it out in a text editor, the reason why emerges:\n",
    "\n",
    "![alt text](./both-tabs-and-commas-screencap.png \"\")\n",
    "\n",
    "Tab delimiters and comma delimiters are two of the most common (if not *the* most common) ways of storing tabular data in a text file. Rather than choose one format or the other, the PLUTO compilers appear to have chosen...both.\n",
    "\n",
    "The following (very inefficient) loop cleans this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_floats_and_whitespace_strings_to_floats_and_strings(series):\n",
    "    l = []\n",
    "    for entry in [str(entry).strip() for entry in series]:\n",
    "        if entry == \"\":\n",
    "            l.append(np.nan)\n",
    "        else:\n",
    "            try:\n",
    "                l.append(float(entry))\n",
    "            except ValueError:\n",
    "                l.append(entry)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_needing_fixing = rolling_pluto_rpad.columns\n",
    "for column in columns_needing_fixing:\n",
    "    rolling_pluto_rpad[column] = convert_floats_and_whitespace_strings_to_floats_and_strings(rolling_pluto_rpad[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rolling_pluto_rpad['CT2010'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rolling_pluto_rpad['CB2010'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a data visualization component to the project I'm applying this fix to the other datasets as well and saving the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dataset in [rolling_sales_agglom, pluto_data_agglom]:\n",
    "    for column in getattr(dataset, 'columns'):\n",
    "        dataset[column] = convert_floats_and_whitespace_strings_to_floats_and_strings(dataset[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Borough        object\n",
       "Block         float64\n",
       "Lot           float64\n",
       "CD            float64\n",
       "CT2010        float64\n",
       "CB2010        float64\n",
       "SchoolDist    float64\n",
       "Council       float64\n",
       "ZipCode       float64\n",
       "FireComp       object\n",
       "               ...   \n",
       "YCoord        float64\n",
       "ZoneMap        object\n",
       "ZMCode         object\n",
       "Sanborn        object\n",
       "TaxMap        float64\n",
       "EDesigNum      object\n",
       "APPBBL        float64\n",
       "APPDate        object\n",
       "PLUTOMapID    float64\n",
       "Version        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pluto_data_agglom.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rolling_sales_agglom.to_csv(\"nyc_rolling_sales.csv\")\n",
    "pluto_data_agglom.to_csv(\"nyc_pluto.csv\")\n",
    "rpad_data_agglom.to_csv(\"nyc_rpad.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to split the dataset into two partitions. The *predictor partition* contains all of our records with associated sales data&mdash;the ground truths on which we will build our model. The *predicted partition* contains all of the fresh records (the vast majority) which do not have sales data associated with them.\n",
    "\n",
    "Once we are satisfied with the power of our model (constructed using the predictor data) we will apply it to the rest of the city (predicted data) and visualize the results for explanatory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_p_pre = rolling_pluto_rpad[rolling_pluto_rpad['SalePrice'].notnull()]\n",
    "r_p_post = rolling_pluto_rpad[rolling_pluto_rpad['SalePrice'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enrich the dataset with calculated variables for market value by square footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ignore the warning.\n",
    "mkt_sqft_values = r_p_pre['SalePrice'] / r_p_pre['BldgArea']\n",
    "r_p_pre['MarketValueSqFt'] = mkt_sqft_values\n",
    "r_p_post['MarketValueSqFt'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thusly also for `CurFvT`, `NewFVT`, and `CuravtA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ignore the warning.\n",
    "for partition in [r_p_pre, r_p_post]:    \n",
    "    assessed_sqft_values = partition['CuravtA'] / partition['BldgArea']\n",
    "    pre_assessed_mkt_values = partition['CurFvT'] / partition['BldgArea']\n",
    "    post_assessed_mkt_values = partition['NewFvT'] / partition['BldgArea']\n",
    "    partition['AssessmentValueSqFt'] = assessed_sqft_values\n",
    "    partition['EstPriorMarketValueSqFt'] = pre_assessed_mkt_values\n",
    "    partition['EstCurentMarketValueSqFt'] = post_assessed_mkt_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_p_pre.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the indices (this isn't strictly necessary, but the resulting data is cleaner and since we can concatenate whilst ignoring indices it won't cause any problems down the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_p_pre.reset_index(drop=True, inplace=True)\n",
    "r_p_post.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving to `csv`, we need to do one more thing: name the `Index`. Otherwise this column's header will not be populated.\n",
    "\n",
    "Note that an `Index` is a `pandas` requirement, not a `csv` one. We could remove it entirely, but don't really gain anything from doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_p_pre.index.name = r_p_post.index.name = 'Index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the true value of the building to be somewhat close to the approximate value calculated by the city. In reality there are a lot of cases where this public figure is as much 50% off or even 100% off, but after a certain point the source of the error becomes not the city's mis-assessment but a below-market-value sale on the part of the selling parties.\n",
    "\n",
    "Here are the ten worst offenders, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_p_pre.sort_values(by = 'SalePrice', ascending=False).tail(10)[::-1][['SalePrice', 'Address', 'AssessmentValueSqFt',\n",
    "                                                                       'EstPriorMarketValueSqFt',\n",
    "                                                                       'EstCurentMarketValueSqFt', 'MarketValueSqFt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple K wouldn't buy you a good used car, let alone a house. We already earlier filtered out the bulk of the in-family or contractual transfers by specifying a lower real estate limit of 1000$. However this wasn't an aggressive enough culling to deal with cases in which the house was sold for real money, just not real house money, for whatever reason.\n",
    "\n",
    "On the other end of the spectrum, property with a high value (especially skyscrapers) tend to be worth more than city estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_p_pre.sort_values(by = 'SalePrice', ascending=False).head(10)[['SalePrice', 'Address', 'AssessmentValueSqFt',\n",
    "                                                                       'EstPriorMarketValueSqFt',\n",
    "                                                                       'EstCurentMarketValueSqFt', 'MarketValueSqFt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a rank distribution of over and under -estimates. Looks like the city's model does decently well, actually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_ratio = (r_p_pre['EstCurentMarketValueSqFt'] / r_p_pre['MarketValueSqFt']).sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_ratio.plot(logy=True, figsize=(24, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of records are accurate to within plus/minus 25%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(m_ratio[(m_ratio > 0.75) & (m_ratio < 1.25)]) / len(m_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go&mdash;a number to try and beat.\n",
    "\n",
    "Here's a plot of market value versus predicted value ranked by market value:\n",
    "\n",
    "A little more information about our dataset before we select a band: is under to over -estimation somehow correlated with actual market value, as I assume is the case (it's easier to be wrong about a big building than a small one)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_p_pre_s = r_p_pre.copy()\n",
    "r_p_pre_s['ValueRatio'] = r_p_pre['EstCurentMarketValueSqFt'] / r_p_pre['MarketValueSqFt']\n",
    "r_p_pre_s['ASE'] = (r_p_pre['EstCurentMarketValueSqFt'] - r_p_pre['MarketValueSqFt'])**2\n",
    "r_p_pre_s.sort_values(by='SalePrice').reset_index().plot(figsize=(24, 10), y='ASE', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the city's model does best when predicting intermediate varibles, and low and high values cause the error to rise. If things seem flat, they're not: remember that this is a logorithmic plot!\n",
    "\n",
    "Looking at a wrongness ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(r_p_pre_s['ValueRatio']).plot(figsize=(24, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't make picking a good cutoff easier; mostly it just shows that there are some really extreme outliers out there. At what point do we stop losing missinformation and start losing the real thing? One more look to decide..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series({n: len(r_p_pre_s[r_p_pre_s['ValueRatio'] > n]) for n in np.linspace(0, 20)}).plot(figsize=(24, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_p_pre_s['OffRatio'] =  ((np.abs(r_p_pre_s['EstCurentMarketValueSqFt'] - r_p_pre_s['MarketValueSqFt'])) / r_p_pre_s['MarketValueSqFt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to be very conservative and band at no more than two standard deviations/the 1% percentile of the data&mdash;corresponding with a cutoff of `'OffRatio' = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(r_p_pre_s[r_p_pre_s['ValueRatio'] > 10]) / len(r_p_pre_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_p_pre_s = r_p_pre_s[r_p_pre_s['ValueRatio'] < 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot again afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((r_p_pre_s['EstCurentMarketValueSqFt']) / r_p_pre_s['MarketValueSqFt']).plot(figsize=(24, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may filter again later into the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that remains is deleting our temporary columns and saving the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_p_pre_s = r_p_pre_s[r_p_pre.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_p_pre_s.to_csv('nyc_building_sales.csv')\n",
    "r_p_post.to_csv('nyc_building_nonsales.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
